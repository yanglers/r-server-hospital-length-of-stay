{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e50443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 complete: data cleaned (numeric->mean, categorical->mode).\n",
      "Step 2 complete: standardized selected labs and created number_of_issues.\n",
      "Split complete: train=70000 rows, test=30000 rows.\n",
      "[RF] MAE=0.4845 | RMSE=0.7650 | R^2=0.8945\n",
      "[RF] OOB R^2=0.8898\n",
      "[GBT] MAE=0.3638 | RMSE=0.4786 | R^2=0.9587\n",
      "[FastTrees] MAE=0.3087 | RMSE=0.4241 | R^2=0.9676\n",
      "[NN] MAE=0.3155 | RMSE=0.4541 | R^2=0.9628\n",
      "\n",
      "Summary metrics (sorted by RMSE):\n",
      "       model       MAE      RMSE        R2\n",
      "2  FastTrees  0.308676  0.424117  0.967560\n",
      "3         NN  0.315546  0.454107  0.962810\n",
      "1        GBT  0.363835  0.478562  0.958697\n",
      "0         RF  0.484519  0.764965  0.894467\n",
      "\n",
      "Predictions tables written to ./predictions_sklearn_los/\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Packages and setup\n",
    "# If needed in a notebook: \n",
    "# !pip install pandas numpy scikit-learn joblib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "RANDOM_STATE = 100\n",
    "TRAIN_PCT = 0.70\n",
    "file_path = Path(\"..\") / \"Data\" / \"LengthOfStay.csv\"  # adjust if needed\n",
    "\n",
    "TARGET = \"lengthofstay\"\n",
    "ID_COL = \"eid\"\n",
    "DATE_COLS = [\"vdate\", \"discharged\"]\n",
    "DROP_FROM_FEATURES = [\"eid\", \"vdate\", \"discharged\", \"facid\"]  # match original exclusions\n",
    "\n",
    "# Columns standardized in the original notebook (Step 2)\n",
    "CONTINUOUS_TO_STANDARDIZE = [\n",
    "    \"hematocrit\", \"neutrophils\", \"sodium\", \"glucose\", \"bloodureanitro\",\n",
    "    \"creatinine\", \"bmi\", \"pulse\", \"respiration\"\n",
    "]\n",
    "\n",
    "# Indicator columns used to compute \"number_of_issues\" (Step 2)\n",
    "ISSUE_INDICATORS = [\n",
    "    \"hemo\", \"dialysisrenalendstage\", \"asthma\", \"irondef\", \"pneum\",\n",
    "    \"substancedependence\", \"psychologicaldisordermajor\", \"depress\",\n",
    "    \"psychother\", \"fibrosisandother\", \"malnutrition\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Utility: evaluation\n",
    "# -----------------------------\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)   # no 'squared' kwarg on older sklearn\n",
    "    rmse = sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"[{model_name}] MAE={mae:.4f} | RMSE={rmse:.4f} | R^2={r2:.4f}\")\n",
    "    return {\"model\": model_name, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load, type cast, clean NA\n",
    "# -----------------------------\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Parse dates (assumes mm/dd/yyyy or similar; coerce invalid to NaT)\n",
    "for c in DATE_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "# Make sure indicator columns exist and are numeric\n",
    "present_issue_cols = [c for c in ISSUE_INDICATORS if c in df.columns]\n",
    "for c in present_issue_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Identify columns to clean (mirror original assumption: no NA clean on eid/lengthofstay/dates)\n",
    "protected = {ID_COL, TARGET, *DATE_COLS}\n",
    "cols_to_consider = [c for c in df.columns if c not in protected]\n",
    "\n",
    "# Split by dtype to impute like original: numeric -> mean, categorical -> mode\n",
    "num_cols_all = df[cols_to_consider].select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols_all = df[cols_to_consider].select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Fill numeric with mean\n",
    "if num_cols_all:\n",
    "    means = df[num_cols_all].mean(numeric_only=True)\n",
    "    df[num_cols_all] = df[num_cols_all].fillna(means)\n",
    "\n",
    "# Fill categorical with mode\n",
    "for c in cat_cols_all:\n",
    "    if df[c].isna().any():\n",
    "        mode_val = df[c].mode(dropna=True)\n",
    "        if not mode_val.empty:\n",
    "            df[c] = df[c].fillna(mode_val.iloc[0])\n",
    "        else:\n",
    "            df[c] = df[c].fillna(\"UNKNOWN\")\n",
    "\n",
    "print(\"Step 1 complete: data cleaned (numeric->mean, categorical->mode).\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Feature engineering\n",
    "# - Standardize selected continuous vars using full-dataset mean/std (as in original)\n",
    "# - Create number_of_issues then convert to string (categorical)\n",
    "# -----------------------------\n",
    "# Standardize selected continuous features (on full dataset, matching original behavior)\n",
    "present_cont = [c for c in CONTINUOUS_TO_STANDARDIZE if c in df.columns]\n",
    "for c in present_cont:\n",
    "    std = df[c].std(ddof=0)\n",
    "    mean = df[c].mean()\n",
    "    # avoid divide-by-zero\n",
    "    df[c] = (df[c] - mean) / (std if std and std != 0 else 1.0)\n",
    "\n",
    "# number_of_issues = sum of indicator columns (after cleaning)\n",
    "if present_issue_cols:\n",
    "    df[\"number_of_issues\"] = df[present_issue_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).sum(axis=1).astype(int)\n",
    "    # Convert to string to mirror original (varchar(2))\n",
    "    df[\"number_of_issues\"] = df[\"number_of_issues\"].astype(str)\n",
    "else:\n",
    "    df[\"number_of_issues\"] = \"0\"\n",
    "\n",
    "# Ensure target is numeric\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "print(\"Step 2 complete: standardized selected labs and created number_of_issues.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare features/target and split\n",
    "# -----------------------------\n",
    "feature_cols = [c for c in df.columns if c not in set(DROP_FROM_FEATURES + [TARGET])]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# Identify categorical vs numeric for ColumnTransformer\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=TRAIN_PCT, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Split complete: train={len(X_train)} rows, test={len(X_test)} rows.\")\n",
    "\n",
    "# ColumnTransformer: One-hot encode categoricals, pass numeric through\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Train & evaluate models\n",
    "# -----------------------------\n",
    "results = []\n",
    "oob_min_samples_split = int(max(2, round(sqrt(len(X_train)))))\n",
    "\n",
    "models = {\n",
    "    \"RF\": RandomForestRegressor(\n",
    "        n_estimators=40,\n",
    "        random_state=5,\n",
    "        oob_score=True,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        min_samples_split=oob_min_samples_split\n",
    "    ),\n",
    "    \"GBT\": GradientBoostingRegressor(\n",
    "        n_estimators=40,\n",
    "        learning_rate=0.3,\n",
    "        random_state=9\n",
    "    ),\n",
    "    \"FastTrees\": HistGradientBoostingRegressor(\n",
    "        max_iter=100,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \"NN\": MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 64),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        max_iter=500,\n",
    "        random_state=17\n",
    "    )\n",
    "}\n",
    "\n",
    "fitted_pipelines = {}\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    fitted_pipelines[name] = pipe\n",
    "\n",
    "    # Metrics\n",
    "    res = evaluate_model(y_test, y_pred, name)\n",
    "    # For RF, also print OOB if available\n",
    "    if name == \"RF\" and hasattr(pipe.named_steps[\"model\"], \"oob_score_\"):\n",
    "        print(f\"[RF] OOB R^2={pipe.named_steps['model'].oob_score_:.4f}\")\n",
    "    results.append(res)\n",
    "\n",
    "# Collate results\n",
    "metrics_df = pd.DataFrame(results).sort_values(\"RMSE\")\n",
    "print(\"\\nSummary metrics (sorted by RMSE):\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Optionally persist models\n",
    "out_dir = Path(\"./models_sklearn_los\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "for name, pipe in fitted_pipelines.items():\n",
    "    joblib.dump(pipe, out_dir / f\"{name}_model.joblib\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build a predictions table like the original (using the GBT model by default)\n",
    "# Includes eid, vdate, observed lengthofstay, rounded prediction, predicted discharge date\n",
    "# -----------------------------\n",
    "def build_predictions_table(model_key=\"GBT\"):\n",
    "    assert model_key in fitted_pipelines, f\"Unknown model_key {model_key}\"\n",
    "    pipe = fitted_pipelines[model_key]\n",
    "\n",
    "    # Run on test set to match evaluation\n",
    "    pred = pipe.predict(X_test)\n",
    "    pred_round = np.rint(pred).astype(int)\n",
    "\n",
    "    # Compose table similar to the SQL SELECT (include key covariates if available)\n",
    "    include_cols = [\n",
    "        \"eid\", \"vdate\", \"rcount\", \"gender\",\n",
    "        \"dialysisrenalendstage\", \"asthma\", \"irondef\", \"pneum\", \"substancedependence\",\n",
    "        \"psychologicaldisordermajor\", \"depress\", \"psychother\", \"fibrosisandother\",\n",
    "        \"malnutrition\", \"hemo\", \"hematocrit\", \"neutrophils\", \"sodium\",\n",
    "        \"glucose\", \"bloodureanitro\", \"creatinine\", \"bmi\", \"pulse\",\n",
    "        \"respiration\", \"number_of_issues\", \"secondarydiagnosisnonicd9\",\n",
    "        \"discharged\", \"facid\", \"lengthofstay\"\n",
    "    ]\n",
    "    present_cols = [c for c in include_cols if c in df.columns]\n",
    "\n",
    "    base = df.loc[X_test.index, present_cols].copy()\n",
    "    # Ensure vdate is datetime\n",
    "    if \"vdate\" in base.columns and not np.issubdtype(base[\"vdate\"].dtype, np.datetime64):\n",
    "        base[\"vdate\"] = pd.to_datetime(base[\"vdate\"], errors=\"coerce\")\n",
    "\n",
    "    base[\"lengthofstay_Pred\"] = pred\n",
    "    base[\"lengthofstay_Pred_Rounded\"] = pred_round\n",
    "    if \"vdate\" in base.columns:\n",
    "        base[\"discharged_Pred\"] = base[\"vdate\"] + pd.to_timedelta(base[\"lengthofstay_Pred_Rounded\"], unit=\"D\")\n",
    "    else:\n",
    "        base[\"discharged_Pred\"] = pd.NaT\n",
    "\n",
    "    # Nice ordering\n",
    "    order = [\"eid\", \"vdate\", \"lengthofstay\", \"lengthofstay_Pred\", \"lengthofstay_Pred_Rounded\", \"discharged_Pred\"]\n",
    "    order += [c for c in present_cols if c not in order]\n",
    "    cols_final = [c for c in order if c in base.columns]\n",
    "    return base[cols_final]\n",
    "\n",
    "los_predictions_gbt = build_predictions_table(\"GBT\")\n",
    "los_predictions_rf = build_predictions_table(\"RF\")\n",
    "los_predictions_fast = build_predictions_table(\"FastTrees\")\n",
    "los_predictions_nn = build_predictions_table(\"NN\")\n",
    "\n",
    "# Save predictions if desired\n",
    "out_dir_preds = Path(\"./predictions_sklearn_los\")\n",
    "out_dir_preds.mkdir(parents=True, exist_ok=True)\n",
    "los_predictions_gbt.to_csv(out_dir_preds / \"LoS_Predictions_GBT.csv\", index=False)\n",
    "los_predictions_rf.to_csv(out_dir_preds / \"LoS_Predictions_RF.csv\", index=False)\n",
    "los_predictions_fast.to_csv(out_dir_preds / \"LoS_Predictions_FastTrees.csv\", index=False)\n",
    "los_predictions_nn.to_csv(out_dir_preds / \"LoS_Predictions_NN.csv\", index=False)\n",
    "\n",
    "print(\"\\nPredictions tables written to ./predictions_sklearn_los/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ayiyang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
